---
layout: post
title: "What is LLM?"
---


> **"Whatever you do, begin by loving yourself. Never forget that such an attitude is the most powerful force in dreaming of the future."**
>
> — *Friedrich Nietzsche*



# What is Large Language Model (LLM) like ChatGPT?



- Large Language Model: Language Model (AI system with "language" data (human-like text)) that is "Large"

- "Large":
  - "Large" in terms of **Scale of Data**: AI models trained on enormous datasets, including texts from books, articles, websites, etc.
  - "Large" in terms of **Number of Parameters**: AI models contain huge number of parameters to be trained.




**Table 1: Summary of well-known large language models (earlier models released before 2024)**

| Model      | Parameter Count       | Training Data Scale      | Company  | Release Date |
|------------|-----------------------|--------------------------|----------|--------------|
| GPT‑2      | ~1.5B                 | ~40GB of internet text   | OpenAI   | 2019         |
| GPT‑3      | ~175B                 | ~300B tokens             | OpenAI   | 2020         |
| GPT‑4      | ~1.7–1.8T (estimated) | ~13T tokens (estimated)  | OpenAI   | 2023         |
| LLaMA      | 7B,13B,33B,65B        | ~1.4T tokens             | Meta     | 2023         |
| PaLM       | ~540B                 | ~780B tokens             | Google   | 2022         |
| Chinchilla | ~70B                  | ~1.4T tokens             | DeepMind | 2022         |


<div style="border-left: 4px solid #007ACC; padding-left: 10px; margin: 10px 0; background-color: #f9f9f9; font-size: 0.7em;">
  <strong>GPT‑1</strong><br>
  GPT‑1 is the very first language model proposed by OpenAI. Although it is modest in size—with only 117M parameters and 4.5GB of training data—it was considered a large language model at the time of its release. By today’s standards, its scale would not qualify as large, but GPT‑1 broke new ground in natural language processing and set the stage for the evolution of modern large language models.
</div>



## References 

### Estimates on GPT-4 parameters and training dataset

1. **Exploding Topics – Number of Parameters in GPT‑4 (Latest Data)**  
   This page summarizes various expert estimates for GPT‑4’s parameter count (around 1.8T) and discusses how these numbers were derived from leaked details and community analysis.  
   [Exploding Topics](https://explodingtopics.com/blog/gpt-parameters)

2. **PlainSwipe – GPT‑4: Details Leaked**  
   An in‐depth look at GPT‑4, including estimates of approximately 1.8 trillion parameters based on its mixture-of-experts architecture and other leaked technical details.  
   [PlainSwipe](https://plainswipe.com/gpt-4-details-leaked/index.html)

3. **Seifeur – Unlocking the Mystery: The Estimated Parameter Count of GPT‑4**  
   This article outlines the reasoning behind the estimated 1.76–1.8T parameter count for GPT‑4 and explores the implications of such scale.  
   [Seifeur](https://seifeur.com/gpt-4-estimated-parameters/)

4. **Wikipedia – GPT‑4**  
   The GPT‑4 Wikipedia page compiles available estimates—including parameter count and training data insights—from multiple sources and leaks.  
   [GPT‑4 on Wikipedia](https://en.wikipedia.org/wiki/GPT-4)

 
