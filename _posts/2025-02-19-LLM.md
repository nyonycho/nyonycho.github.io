---
layout: post
title: "What is LLM?"
---

# What is Large Language Model (LLM) like ChatGPT?

- "Large":
  - "Large" in terms of **Scale of Data**: AI models trained on enormous datasets, including texts from books, articles, websites, etc.
  - "Large" in terms of **Number of Parameters**: AI models contain huge number of parameters to be trained.

**Table 1: Summary of well-known large language models**

| Model         | Parameter Count                          | Training Data Scale                                                  | Additional Notes                                  |
|---------------|------------------------------------------|----------------------------------------------------------------------|---------------------------------------------------|
| **GPT-2**     | ~1.5 billion (largest variant)           | ~40GB of internet text (WebText dataset)                             | Released in 2019                                  |
| **GPT-3**     | ~175 billion                             | ~300 billion tokens (sourced from Common Crawl, Books, Wikipedia, etc.) | Pioneered few-shot learning capabilities         |
| **GPT-4**     | Undisclosed                              | Undisclosed                                                          | Believed to use more curated and larger-scale data|
| **LLaMA**     | 7B, 13B, 33B, 65B                         | ~1.4 trillion tokens (aggregated from diverse public datasets)         | Released by Meta in 2023                           |
| **PaLM**      | ~540 billion                             | ~780 billion tokens                                                  | Developed by Google using the Pathways system      |
| **Chinchilla**| ~70 billion                              | ~1.4 trillion tokens                                                 | Designed for compute-optimal scaling (DeepMind)    |
